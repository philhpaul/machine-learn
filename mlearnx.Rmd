---
title: "Machine Learning -- Course Project"
author: "phillip paul"
date: "Tuesday, March 17, 2015"
output: word_document
---

```{r setup, include=FALSE}
require(caret)   #  training functions
require(corrplot)  # correlation plots
require(randomForest)
setInternet2(TRUE)
```

Machine Learning Project: Train a model to predict correct weight lifting activity.

Computations done under: Win7 running Rx64 3.1.2 and using RStudio 0.98.109

Original data from: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013

```{r specs, include=FALSE}
trainURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
rawtrain<-read.csv(url(trainURL), na.strings=c("NA","#DIV/0!",""))
rawtest<-read.csv(url(testURL), na.strings=c("NA","#DIV/0!","")) # test set
# rawtrain is composed of data windows, each window has columns with basic data 
#   and columns with single values parameters computed over the window.  
#   Such columns are over 90% NA and are ignored in this study
fracNA <- colSums(is.na(rawtrain))/nrow(rawtrain) # fraction of NA in a column
# rawtrain also has a number on non-data columns, specifically
# col 1 (row number) 
# col 2 (subject name) 
# col 3:7 (time stamps and windows state ID) 
fracNA[1:7]<-1.0 # mark the not-data as having too much NA so as to que to ignor
cutNA <- which(fracNA < 0.66) # anything with more than 2/3 NA is to ignor  
cTrain<-rawtrain[,cutNA]  # cleaned-up training set
# check cTrain for remaining NA
anyNA<-unique(fracNA[cutNA]) # returns only 0 indicating no NA in this set
# check cTrain for near-zero-variance
cTrainNZV <- nearZeroVar(cTrain, saveMetrics=TRUE)  # returns all nzv=F
```

In the origional study, subjects executed weight curls in various defined fashions, correctly (class A) and variants of incorrectly (classes B - E). Subjects were instrumented with inertial sensors to document these activities. For reference, the geometry of the sensor orientation was reported as set to: y - vertical, z - subject line-of-sight and x - right-hand-rule-true to y-z.

Here, the objective is to develop a model that takes the instrumentation records as input and determines correct versus incorrect and more generally identify particular incorrect behavior. In effect, to distinguish between the 5 classes.  

The origional data set is composed of some number of repeats of the 5 classes of exercises (weight curls) done by 6 subjects. Some portion was held back (reserved) for final testing. The  data set contains record windows (each a series of measurements for a repeat-class-subject) of data from 3-axis magnetometer, accelerometer and gyroscopic sensors as mounted on the subject's front-center-belt, upper arm  and active wrist (termed forearm in the data set) and on the dumbbell. The set also contains point-by-point processed values for each sensor, specifically the Euler angles (roll, pitch and yaw) and total acceleration. These processed values are non-linear combinations of the basic data and may a preferred format or may be redundant in model generation. The data set also contains a number of summary parameters for each window. For purposes of the study reported here, these summary parameters are ignored. 

The "cleaned" set is then composed of 52 predictors and 1 target. The predictors of the set are complete (free of NA entries) and have acceptable variance (pass an NZV test).  

The training set is further segmented to 70% for actual model training and 30% for model validation and error estimation.
 
```{r calcA, include=FALSE}
# partition clean set into train/validate   
set.seed(314)
inTrain<-createDataPartition(y=cTrain$classe, p=0.7, list=FALSE)
mtrain<-cTrain[inTrain, ]  # 70% for training
mtest<-cTrain[-inTrain, ] # 30% for validation and OOS error estimate
modelVs<-colnames(mtrain) # list of names in set 
classeID<-grep("classe",modelVs) # col location of "classe"
#
# apply randomForest to mtrain with predictors modelVs to predict classe
set.seed(31415)
modFit<-randomForest(classe ~., data=mtrain, type="class")
# check in-sample
ptrain<-predict(modFit,newdata=mtrain)  # in-sample 
confuseIS<-confusionMatrix(ptrain,mtrain$classe)  # returns a diagonal 
accurateIS<-confuseIS$overall['Accuracy']  # returns 100%
#
# validate and get an error estimate
ptest<-predict(modFit,newdata=mtest[,-classeID],type="class") # out-of-sample
confuseOOS<-confusionMatrix(ptest,mtest$classe) 
accurateOOS<-confuseOOS$overall['Accuracy']  #returns 0.9952421
```

A random forest algorithm was applied to the training set segment. The in-sample confusion matrix was seen to be diagonal with a model accuracy of 100%. The model was applied to the reserved validation set. For out-of-sample, the accuracy is over 99.5% and the detailed result statistics are: 

```{r printA, echo=FALSE}
print(confuseOOS)
```

A CART model (using the rpart function in R) was evaluated in a like fashion, returned an out-of-sample accuracy of about 77% and was abandoned in favor of the random forest approach. 

Highly correlated and highly anti-correlated predictors are essentially redundant.
Of the 52 predictors: 31, 25 and 20 have magnitudes-of-correlation in excess of 0.5, 0.66 and 0.75, respectively. To reduce potential over-fitting and to improve model training speed it is preferable to optimize the number of predictors. Reducing the number of predictors generally decreases variance at the cost of possibly adding some bias.  

A reduced model was formed by excluding about 1/2 of the predictors (taking a correlation cut-off of 0.66). The heat-map of the correlation matrix for the 27 retained predictors (the map also lists the identities of these predictors) is shown below. In the heat map: more blue-colored or more red-colored indicates more correlated or anti-correlated pairs respectively, whereas a white color indicates un-correlated pairs.

```{r correl, include=FALSE}
#
# check to see if there are redundant (highly correlated) predictors
correls<-cor(mtrain[,-dim(mtrain)[2]],) # full correlation matrix
# the training set has 52 variables (predictors) and classe
# corr cuttoff of 0.5, 0.66, 0.75 finds 31, 25, 20 items to remove, respectively
# pick cuttoff = 0.66 to remove about 1/2 the predictors
verycorrel <- findCorrelation(correls, cutoff = 0.66) 
newtrain <- mtrain[,-verycorrel]
correls<-cor(newtrain[,-dim(newtrain)[2]],)  #look at resulting matrix
# use a color ramp taken from CRAN / An Introduction to corrplot package
col1 <- colorRampPalette(c("#7F0000", "red", "#FF7F00", "yellow", "white", "cyan", 
                           "#007FFF", "blue", "#00007F"))
```

```{r plotcorrel, echo=FALSE}
corrplot(correls, method = "color", type="lower", order="original", 
         tl.cex = 0.66, tl.col="black", tl.srt = 45, col=col1(100))
```

```{r calcB, include=FALSE}
set.seed(31415)
modFit3<-randomForest(classe ~., data=newtrain, type="class")
ptest3<-predict(modFit3,newdata=mtest[,-classeID],type="class") # out-of-sample
confuseOOS3<-confusionMatrix(ptest3,mtest$classe)
# returns 98.886% so out-of-sample error is about 1.2% with 28 predictors
# compared to 99.952% using all 52 predictors
accurateOOS3<-confuseOOS3$overall['Accuracy']  #returns 0.9892948
```

A random forest algorithm was applied to the reduced training set (27 predictors). This new model was applied to the reserved validation set. For out-of-sample the new model accuracy is over 98.9% and the detailed result statistics are: 

```{r printB, echo=FALSE}
print(confuseOOS3)
```

The sensitivity, selectivity, and accuracy of this new model (based on 27 select predictors) compares favorably with the full model (based on 52 predictors). The new model is the result of this study and was applied to successfully predict all elements of the formal (reserved) test set. 

```{r predfinal, include=FALSE}
# predict the final test set
predTestSet3<-predict(modFit3, newdata = rawtest, type="class")
answers<-as.character(predTestSet3)  # convert to a char vector
#
# write output in the prescribed fashion
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
# change working directory to target directory...
## pml_write_files(answers)
```

In summary: A random forest model using 27 predictors (as selected out of 52 by eliminating elements with over 66% correlation) provided an out-of-sample sensitivity and selectivity in excess of 97.5% for all 5 classes, and correctly predicted 100% of the final test cases. It is possible that a further reduction in the number of predictors is possible. The random forest approach was found to be superior to a CART model approach. 

The reduced set of 27 predictors are: 5 from the belt sensor as x-y-z gyro and x-z magnet.; 5 from the dumbbell sensor as 3 Euler angles, z gyro and z magnet.; 8 from the arm sensor as 3 Euler angles, total acceleration, y-z gyro and y-z magnet.; 9 from the wrist sensor as 3 Euler angles, total acceleration, x gyro, z acceleration and x-y-z magnet. Of these, the 9 Euler angles, arm gyro-z, wrist acceleration-z and the 8 magnetometer measures are found to be the more important predictors. It appears that relative position and direction sensor data (magnetometer and gyro components) are more useful than acceleration. The relative importance of the magnetometer signal raises a concern, specifically the use of magnetometer sensors without stated (in the study report) controls for disruptive magnetic entities (e.g. the dumbbell used in the study). 

The eventual target for the type of model presented here is likely a deployable (e.g. man-portable) training device. In this regard: 1) The magnetometer signals were found to be important. However, use of a magnetometer may be precluded by interference. 2) It will be preferable to use down-sampled measures to optimize available computing power. Down-sampled (summary windowed) parameters were provided in the original data set. However these summary values could not be properly and meaningfully related back to the respective data traces. For pure interest and in a separate effort, a set of down-sampled predictors were computed from the data traces. A random forest model was trained to these new summary predictors. This 'summary' model had a predictive power that compared favorably to the results reported here. 

  
 